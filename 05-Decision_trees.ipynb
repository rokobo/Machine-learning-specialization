{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ebd129",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement 1**\n",
    "\n",
    "You are starting a company that grows and sells wild mushrooms. Since not all mushrooms are edible, you'd like to be able to tell whether a given mushroom is edible or poisonous based on it's physical attributes. You have some existing data that you can use for this task:\n",
    "\n",
    "|                                                             | Cap Color | Stalk Shape | Solitary | Edible |\n",
    "|:-----------------------------------------------------------:|:---------:|:-----------:|:--------:|:------:|\n",
    "| <img src=\"images/mushroom_0.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |   Tapering  |    Yes   |    1   |\n",
    "| <img src=\"images/mushroom_1.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    Yes   |    1   |\n",
    "| <img src=\"images/mushroom_2.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    No    |    0   |\n",
    "| <img src=\"images/mushroom_3.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    No    |    0   |\n",
    "| <img src=\"images/mushroom_4.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |   Tapering  |    Yes   |    1   |\n",
    "| <img src=\"images/mushroom_5.png\" alt=\"drawing\" width=\"50\"/> |    Red    |   Tapering  |    Yes   |    0   |\n",
    "| <img src=\"images/mushroom_6.png\" alt=\"drawing\" width=\"50\"/> |    Red    |  Enlarging  |    No    |    0   |\n",
    "| <img src=\"images/mushroom_7.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    Yes   |    1   |\n",
    "| <img src=\"images/mushroom_8.png\" alt=\"drawing\" width=\"50\"/> |    Red    |   Tapering  |    No    |    1   |\n",
    "| <img src=\"images/mushroom_9.png\" alt=\"drawing\" width=\"50\"/> |   Brown   |  Enlarging  |    No    |    0   |\n",
    "\n",
    "For ease of implemententation we use one-hot encoding of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_train = np.array([\n",
    "    [1, 1, 1], [1, 0, 1], [1, 0, 0], [1, 0, 0], [1, 1, 1],\n",
    "    [0, 1, 1], [0, 0, 0], [1, 0, 1], [0, 1, 0], [1, 0, 0]\n",
    "])\n",
    "y_train = np.array([1, 1, 0, 0, 1, 0, 0, 1, 1, 0])\n",
    "assert X_train.shape == (10, 3)\n",
    "assert y_train.shape == (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae76ed1",
   "metadata": {},
   "source": [
    "Now we build a decision tree with a stopping criteria of maximum depth of 2. To begin we start by creating a function to calculate the entropy for a given node using the formula:\n",
    "\n",
    "$$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def compute_entropy(y: ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes the entropy for a node.\n",
    "\n",
    "    Args:\n",
    "        y (ndarray): Numpy array indicating whether each example at a node is\n",
    "            dible (`1`) or poisonous (`0`)\n",
    "\n",
    "    Returns:\n",
    "        entropy (float): Entropy at that node.\n",
    "\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    if m == 0:\n",
    "        return 0\n",
    "\n",
    "    p1 = np.sum(y) / m\n",
    "    if (p1 == 1) or (p1 == 0):\n",
    "        return 0\n",
    "\n",
    "    p0 = 1 - p1\n",
    "    entropy = (-p1 * np.log2(p1)) - (p0 * np.log2(p0))\n",
    "    return entropy\n",
    "\n",
    "assert compute_entropy(np.array([1] * 10)) == 0\n",
    "assert compute_entropy(np.array([0] * 10)) == 0\n",
    "assert compute_entropy(np.array([0] * 12 + [1] * 12)) == 1\n",
    "y = np.array([1, 0, 1, 0, 1, 1, 1, 0, 1])\n",
    "assert np.isclose(compute_entropy(-y + 1), compute_entropy(y), atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a helper function called `split_dataset` that takes in the data at a node and a feature to split on and splits it into left and right branches.\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def split_dataset(\n",
    "    X: ndarray, node_indices: list[int], feature: int\n",
    ") -> tuple[list[int], list[int]]:\n",
    "    \"\"\"\n",
    "    Splits the data at the given node into left and right branches.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Data matrix of shape(n_samples, n_features).\n",
    "        node_indices (list[int]): List containing the active indices.\n",
    "            I.e, the samples being considered at this step.\n",
    "        feature (int): Index of feature to split on.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[int], list[int]]: Indices with feature == 1, and == 0.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "\n",
    "    for index in node_indices:\n",
    "        if X[index][feature] == 1:\n",
    "            left_indices.append(index)\n",
    "        else:\n",
    "            right_indices.append(index)\n",
    "\n",
    "    return left_indices, right_indices\n",
    "\n",
    "assert split_dataset(X_train, list(range(10)), 0) == ([0, 1, 2, 3, 4, 7, 9], [5, 6, 8])\n",
    "assert split_dataset(X_train, [0, 2, 4, 6, 8], 0) == ([0, 2, 4], [6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a function called `compute_information_gain()` that takes in the training data, the indices at a node and a feature to split on and returns the information gain from the split:\n",
    "\n",
    "\n",
    "$$\\text{Information Gain} = H(p_1^\\text{node})- (w^{\\text{left}}H(p_1^\\text{left}) + w^{\\text{right}}H(p_1^\\text{right}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def compute_information_gain(\n",
    "    X: ndarray, y: ndarray, node_indices: ndarray, feature: int\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the information of splitting the node on a given feature.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Data matrix of shape(n_samples, n_features).\n",
    "        y (ndarray): Array with n_samples containing the target variable.\n",
    "        node_indices (ndarray): List containing the active indices.\n",
    "            I.e, the samples being considered in this step.\n",
    "\n",
    "    Returns:\n",
    "        cost (float): Cost computed.\n",
    "    \"\"\"\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    m = len(X[node_indices])\n",
    "    root_entropy = compute_entropy(y[node_indices])\n",
    "    w_left = len(left_indices) / m\n",
    "    w_right = len(right_indices) / m\n",
    "    entropy_left = compute_entropy(y[left_indices])\n",
    "    entropy_right = compute_entropy(y[right_indices])\n",
    "    weighted_entropy = (w_left * entropy_left) + (w_right * entropy_right)\n",
    "    information_gain = root_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "X_cig = np.array([[1, 0], [1, 0], [1, 0], [0, 0], [0, 1]])\n",
    "y_cig = np.array([[0, 1, 0, 1, 0]]).T\n",
    "\n",
    "assert compute_information_gain(\n",
    "    X_cig, np.array([[0, 0, 0, 0, 0]]).T, list(range(5)), 0) == 0\n",
    "assert np.isclose(compute_information_gain(\n",
    "    X_cig, y_cig, list(range(5)), 0), 0.019973, atol=1e-6)\n",
    "assert np.isclose(compute_information_gain(\n",
    "    X_cig, y_cig, list(range(5)), 1), 0.170951, atol=1e-6)\n",
    "assert np.isclose(compute_information_gain(\n",
    "    X_cig, y_cig, list(range(4)), 0), 0.311278, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function to get the best feature to split on by computing the information gain from each feature as we did above and returning the feature that gives the maximum information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def get_best_split(X: ndarray, y: ndarray, node_indices: ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Returns the optimal feature to split the node data.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Data matrix of shape(n_samples, n_features)\n",
    "        y (ndarray): Array with n_samples containing the target variable.\n",
    "        node_indices (ndarray): List containing the active indices.\n",
    "            I.e, the samples being considered in this step.\n",
    "\n",
    "    Returns:\n",
    "        best_feature (int): The index of the best feature to split.\n",
    "    \"\"\"\n",
    "\n",
    "    best_feature = -1\n",
    "    best_information_gain = 0\n",
    "\n",
    "    for feature in range(X.shape[1]):\n",
    "        information_gain = compute_information_gain(X, y, node_indices, feature)\n",
    "        if information_gain > best_information_gain:\n",
    "            best_information_gain = information_gain\n",
    "            best_feature = feature\n",
    "    return best_feature\n",
    "\n",
    "X = np.array([[1, 0], [1, 0], [1, 0], [0, 0], [0, 1]])\n",
    "y = np.array([[0, 0, 0, 0, 0]]).T\n",
    "node_indexes = list(range(5))\n",
    "assert get_best_split(X, y, node_indexes) == -1\n",
    "assert get_best_split(X, X[:, 0], node_indexes) == 0\n",
    "assert get_best_split(X, X[:, 1], node_indexes) == 1\n",
    "assert get_best_split(X, 1 - X[:, 0], node_indexes) == 0\n",
    "assert get_best_split(X, np.array([[0, 1, 0, 1, 0]]).T, node_indexes) == 1\n",
    "assert get_best_split(X, np.array([[0, 1, 0, 1, 0]]).T, [2, 3, 4]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the functions implemented above to generate a decision tree by successively picking the best feature to split on until we reach the stopping criteria (maximum depth is 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth 0, Root: Split on feature: 2\n",
      "- Depth 1, Left: Split on feature: 0\n",
      "  -- Left leaf node with indices [0, 1, 4, 7]\n",
      "  -- Right leaf node with indices [5]\n",
      "- Depth 1, Right: Split on feature: 1\n",
      "  -- Left leaf node with indices [8]\n",
      "  -- Right leaf node with indices [2, 3, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "def build_tree_recursive(\n",
    "    X: ndarray, y: ndarray, node_indices: ndarray,\n",
    "    branch_name: str, max_depth: int, current_depth: int\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Build a tree using the recursive algorithm that split the dataset into\n",
    "    2 subgroups at each node. This function just prints the tree.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Data matrix of shape(n_samples, n_features)\n",
    "        y (ndarray): Array with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices.\n",
    "            I.e, the samples being considered in this step.\n",
    "        branch_name (str): Name of the branch. ['Root', 'Left', 'Right']\n",
    "        max_depth (int): Max depth of the resulting tree.\n",
    "        current_depth (int): Current depth. Used during recursive call.\n",
    "    \"\"\"\n",
    "\n",
    "    # Maximum depth reached - stop splitting\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        return\n",
    "\n",
    "    # Otherwise, get best split and split the data\n",
    "    best_feature = get_best_split(X, y, node_indices)\n",
    "\n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
    "\n",
    "    # Split the dataset at the best feature\n",
    "    l_indices, r_indices = split_dataset(X, node_indices, best_feature)\n",
    "    tree.append((l_indices, r_indices, best_feature))\n",
    "\n",
    "    # continue splitting the left and the right child. Increment current depth\n",
    "    build_tree_recursive(X, y, l_indices, \"Left\", max_depth, current_depth+1)\n",
    "    build_tree_recursive(X, y, r_indices, \"Right\", max_depth, current_depth+1)\n",
    "\n",
    "tree = []\n",
    "build_tree_recursive(X_train, y_train, list(range(10)), \"Root\", 2, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
